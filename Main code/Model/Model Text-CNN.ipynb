{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M81n5YJfXhi3"
   },
   "source": [
    "# Library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FRK31_QeOx9r",
    "outputId": "31951e8e-9a3d-4895-c6f5-efe61ad14673"
   },
   "outputs": [],
   "source": [
    "pip install pyvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YeeDYlL-58mU",
    "outputId": "28188e22-5f7a-47a3-ac7e-89b48baf5340"
   },
   "outputs": [],
   "source": [
    "pip install tensorflow-gpu==2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y8uvkIcF7NDJ",
    "outputId": "96620982-b57b-4653-d454-a9f7b1bbeaae"
   },
   "outputs": [],
   "source": [
    "pip install keras==2.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJkUyD1q7x2b"
   },
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UZlqfic28EXR",
    "outputId": "90fc92eb-b00d-417b-9977-793d24ad7764"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "DATA = 'train.csv'\n",
    "DEV_DATA = 'dev.csv'\n",
    "TEST_DATA = 'test.csv'\n",
    "\n",
    "# read data\n",
    "train_data = pd.read_csv(DATA)\n",
    "dev_data = pd.read_csv(DEV_DATA)\n",
    "test_data = pd.read_csv(TEST_DATA)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(dev_data))\n",
    "print(len(test_data))\n",
    "\n",
    "X_train = train_data['free_text']\n",
    "y_train = train_data['label_id'].values\n",
    "\n",
    "X_dev = dev_data['free_text']\n",
    "y_dev = dev_data['label_id'].values\n",
    "\n",
    "X_test = test_data['free_text']\n",
    "y_test = test_data['label_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OgTZiygq7xNq"
   },
   "outputs": [],
   "source": [
    "#pre-process\n",
    "import re\n",
    "import numpy as np\n",
    "import  re\n",
    "import string\n",
    "STOPWORDS = 'vietnamese-stopwords.txt'\n",
    "with open(STOPWORDS, \"r\", encoding=\"utf8\") as ins:\n",
    "    stopwords = []\n",
    "    for line in ins:\n",
    "        dd = line.strip('\\n')\n",
    "        stopwords.append(dd)\n",
    "    stopwords = set(stopwords)\n",
    "#old\n",
    "def filter_stop_words(train_sentences, stop_words):\n",
    "    new_sent = [word for word in train_sentences.split() if word not in stop_words]\n",
    "    train_sentences = ' '.join(new_sent)\n",
    "        \n",
    "    return train_sentences\n",
    "\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "#new\n",
    "def normalText(sent):\n",
    "    sent = str(sent).replace('_',' ').replace('/',' trên ')\n",
    "    sent = re.sub('-{2,}','',sent)\n",
    "    sent = re.sub('\\s+',' ', sent)\n",
    "    patPrice = r'([0-9]+k?(\\s?-\\s?)[0-9]+\\s?(k|K))|([0-9]+(.|,)?[0-9]+\\s?(triệu|ngàn|trăm|k|K|))|([0-9]+(.[0-9]+)?Ä‘)|([0-9]+k)'\n",
    "    patHagTag = r'#\\s?[aăâbcdđeêghiklmnoôơpqrstuưvxyàằầbcdđèềghìklmnòồờpqrstùừvxỳáắấbcdđéếghíklmnóốớpqrstúứvxýảẳẩbcdđẻểghỉklmnỏổởpqrstủửvxỷạặậbcdđẹệghịklmnọộợpqrstụựvxỵãẵẫbcdđẽễghĩklmnõỗỡpqrstũữvxỹAĂÂBCDĐEÊGHIKLMNOÔƠPQRSTUƯVXYÀẰẦBCDĐÈỀGHÌKLMNÒỒỜPQRSTÙỪVXỲÁẮẤBCDĐÉẾGHÍKLMNÓỐỚPQRSTÚỨVXÝẠẶẬBCDĐẸỆGHỊKLMNỌỘỢPQRSTỤỰVXỴẢẲẨBCDĐẺỂGHỈKLMNỎỔỞPQRSTỦỬVXỶÃẴẪBCDĐẼỄGHĨKLMNÕỖỠPQRSTŨỮVXỸ]+'\n",
    "    patURL = r\"(?:http://|www.)[^\\\"]+\"\n",
    "    sent = re.sub(patURL,'website',sent)\n",
    "    sent = re.sub(patHagTag,' hagtag ',sent)\n",
    "    sent = re.sub(patPrice, ' giá_tiền ', sent)\n",
    "    sent = re.sub('\\.+','.',sent)\n",
    "    sent = re.sub('(hagtag\\s+)+',' hagtag ',sent)\n",
    "    sent = re.sub('\\s+',' ',sent)\n",
    "    return sent\n",
    "\n",
    "\n",
    "def normalize_elonge_word(sent):\n",
    "    s_new = ''\n",
    "    for word in sent.split(' '):\n",
    "        word_new = ' '\n",
    "        for char in word.strip():\n",
    "            if char != word_new[-1]:\n",
    "                word_new += char\n",
    "        s_new += word_new.strip() + ' '\n",
    "    return s_new.strip()\n",
    "\n",
    "def deleteIcon(text):\n",
    "    text = text.lower()\n",
    "    s = ''\n",
    "    pattern = r\"[a-zA-ZaăâbcdđeêghiklmnoôơpqrstuưvxyàằầbcdđèềghìklmnòồờpqrstùừvxỳáắấbcdđéếghíklmnóốớpqrstúứvxýảẳẩbcdđẻểghỉklmnỏổởpqrstủửvxỷạặậbcdđẹệghịklmnọộợpqrstụựvxỵãẵẫbcdđẽễghĩklmnõỗỡpqrstũữvxỹAĂÂBCDĐEÊGHIKLMNOÔƠPQRSTUƯVXYÀẰẦBCDĐÈỀGHÌKLMNÒỒỜPQRSTÙỪVXỲÁẮẤBCDĐÉẾGHÍKLMNÓỐỚPQRSTÚỨVXÝẠẶẬBCDĐẸỆGHỊKLMNỌỘỢPQRSTỤỰVXỴẢẲẨBCDĐẺỂGHỈKLMNỎỔỞPQRSTỦỬVXỶÃẴẪBCDĐẼỄGHĨKLMNÕỖỠPQRSTŨỮVXỸ,._]\"\n",
    "\n",
    "    for char in text:\n",
    "        if char !=' ':\n",
    "            if len(re.findall(pattern, char)) != 0:\n",
    "                s+=char\n",
    "            elif char == '_':\n",
    "                s+=char\n",
    "        else:\n",
    "            s+=char\n",
    "    s = re.sub('\\s+',' ',s)\n",
    "    return s.strip()\n",
    "\n",
    "def preprocess(text, tokenized=True, lowercased=True):\n",
    "    for punc in string.punctuation:\n",
    "        text = text.replace(punc,' '+ punc + ' ')\n",
    "    text = normalText(text)\n",
    "    text = deleteIcon(text)\n",
    "    # Lowercase\n",
    "    #text = text.lower()\n",
    "    # Removing multiple whitespaces\n",
    "    text = re.sub(r\"\\?\", \" \\? \", text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"[0-9]+\", \" num \", text)\n",
    "    # Split in tokens\n",
    "    # Remove punctuation\n",
    "    for punc in string.punctuation:\n",
    "      if punc not in \"_\":\n",
    "          text = text.replace(punc,' ')\n",
    "    text = re.sub('\\s+',' ',text)\n",
    "    text = normalize_elonge_word(text)\n",
    "    #text = filter_stop_words(text, stopwords)\n",
    "    #text = deEmojify(text)\n",
    "    #text = filter_stop_words(text, stopwords)\n",
    "    text = text.lower() if lowercased else text\n",
    "    if tokenized:\n",
    "        pre_text = \"\"\n",
    "        sentences = vncorenlp.tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            pre_text += \" \".join(sentence)\n",
    "        text = pre_text\n",
    "\n",
    "    return text\n",
    "\n",
    "def pre_process_features(X, y, tokenized=True, lowercased=True):\n",
    "    X = [preprocess(str(p), tokenized=tokenized, lowercased=lowercased) for p in list(X)]\n",
    "    for idx, ele in enumerate(X):\n",
    "        if not ele:\n",
    "            np.delete(X, idx)\n",
    "            np.delete(y, idx)\n",
    "    return X, y\n",
    "\n",
    "X_train = train['free_text']\n",
    "y_train = train['label_id'].values\n",
    "\n",
    "X_dev = dev['free_text']\n",
    "y_dev = dev['label_id'].values\n",
    "\n",
    "X_test = test['free_text']\n",
    "y_test = test['label_id'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBlkTx2wd7Sj"
   },
   "source": [
    "# Run models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V01ASmKuck6C"
   },
   "source": [
    "## Text CNN (Convolutional neural network for text classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-4cBz1CgdPJy",
    "outputId": "3e0f1cbd-634a-4dea-c479-9b674f513640"
   },
   "outputs": [],
   "source": [
    "# Text CNN \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "train_X, train_y = pre_process_features(X_train, y_train, tokenized=True, lowercased = True)\n",
    "dev_X, dev_y = pre_process_features(X_dev, y_dev, tokenized=True, lowercased = True)\n",
    "test_X, test_y = pre_process_features(X_test, y_test, tokenized=True, lowercased = True)\n",
    "\n",
    "EMBEDDING_FILE = 'cc.vi.300.vec'\n",
    "MODEL_FILE = 'model/Text_CNN_model_v13.h5'\n",
    "\n",
    "def make_featues(X, y, tokenizer, is_one_hot_label=True):\n",
    "    X = tokenizer.texts_to_sequences(X)\n",
    "    X = sequence.pad_sequences(X, maxlen=sequence_length)\n",
    "    if is_one_hot_label: \n",
    "        y = to_categorical(y, num_classes=3)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "vocabulary_size = 10000\n",
    "sequence_length = 100\n",
    "\n",
    "embedding_dim = 300\n",
    "batch_size = 256\n",
    "epochs = 40\n",
    "drop = 0.5\n",
    "\n",
    "filter_sizes = [2,3,5]\n",
    "num_filters = 32\n",
    "\n",
    "# --------------LOAD WORD EMBEDDING -------------------------\n",
    "embeddings_index = {}\n",
    "with open(EMBEDDING_FILE, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "\n",
    "tokenizer = text.Tokenizer(lower=False, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(train_X)\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "# num_words = min(vocabulary_size, len(word_index) + 1)\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= vocabulary_size:\n",
    "        continue\n",
    "\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Make features\n",
    "train_X, train_y = make_featues(train_X, train_y, tokenizer)\n",
    "dev_X, dev_y = make_featues(dev_X, dev_y, tokenizer)\n",
    "test_X, test_y = make_featues(test_X, test_y, tokenizer, is_one_hot_label=False)\n",
    "\n",
    "\n",
    "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "embedding = Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=sequence_length, weights=[embedding_matrix])(inputs)\n",
    "reshape = Reshape((sequence_length,embedding_dim,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='elu')(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='elu')(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='elu')(reshape)\n",
    "\n",
    "maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=3, activation='softmax')(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "checkpoint = ModelCheckpoint('weights.{epoch:03d}-{val_acc:.4f}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_X, train_y, batch_size=batch_size, epochs=epochs, verbose=1, \n",
    "          validation_data=(dev_X, dev_y))  \n",
    "model.save(MODEL_FILE)\n",
    "\n",
    "prediction = model.predict(test_X, batch_size=batch_size, verbose=0)\n",
    "y_pred = prediction.argmax(axis=-1)\n",
    "\n",
    "cf1 = confusion_matrix(test_y, y_pred)\n",
    "print(cf1)\n",
    "\n",
    "evaluation = f1_score(test_y, y_pred, average='micro')\n",
    "\n",
    "print(\"F1 - micro: \" + str(evaluation))\n",
    "\n",
    "evaluation = f1_score(test_y, y_pred, average='macro')\n",
    "print(\"F1 - macro: \" + str(evaluation))\n",
    "\n",
    "evaluation = accuracy_score(test_y, y_pred)\n",
    "print(\"Accuracy: \" + str(evaluation))\n",
    "\n",
    "df_cm1 = pd.DataFrame(cf1, index = [\"clean\",\"offensive\",\"hate\"],\n",
    "                  columns = [\"clean\",\"offensive\",\"hate\"])\n",
    "plt.clf()\n",
    "sn.heatmap(df_cm1, annot=True, cmap=\"Greys\",fmt='g', cbar=True, annot_kws={\"size\": 30})"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "M81n5YJfXhi3"
   ],
   "machine_shape": "hm",
   "name": "ViHSD DNN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
